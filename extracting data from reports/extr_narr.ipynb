{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "#document loaders\n",
    "file_loader= PyPDFLoader\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "#model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "#vector store\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "#splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) \n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## location of docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DIR = './reports/24Q1USBANK'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_PATH = os.path.join(PATH_TO_DIR, \"faiss_index\")\n",
    "if(not os.path.exists(INDEX_PATH)):\n",
    "    docs = []\n",
    "    for f in os.listdir(PATH_TO_DIR):\n",
    "        file_path = os.path.join(PATH_TO_DIR, f)\n",
    "        if os.path.isfile(file_path):\n",
    "            loader = file_loader(file_path)\n",
    "            docs.extend(loader.load())\n",
    "\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    vectorstore = FAISS.from_documents(splits, embedding_model)\n",
    "    vectorstore.save_local(INDEX_PATH)\n",
    "else:\n",
    "    vectorstore = FAISS.load_local(INDEX_PATH, embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# description of Agent task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCRIPTION_OF_PURPOSE = \"\"\"You are a AI agent for extracting the economic predictions from text. Use the given context to summarize information in it\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a AI agent for extracting the economic predictions from text. Use the given context to summarize information in it'), HumanMessage(content='Context:\\nhello')]\n"
     ]
    }
   ],
   "source": [
    "chat_prompt_extr_narr = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(DESCRIPTION_OF_PURPOSE)\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"Context:\\n{context}\"),\n",
    "    ]\n",
    ")\n",
    "print(chat_prompt_extr_narr.invoke({'context': \"hello\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents.base import Document\n",
    "\n",
    "def format_docs(docs: Document) -> str: \n",
    "    return \"\\n------\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableSequence, RunnableAssign, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETRIEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fiddle with this\n",
    "#retriever  = vectorstore.as_retriever()\n",
    "\n",
    "# Only retrieve documents that have a relevance score\n",
    "# Above a certain threshold\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.5}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAIN\n",
    "the current chain just has the documents used and writes out the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnableParallel({\n",
    "    \"context\": retriever\n",
    "    })\n",
    "    | RunnableAssign(\n",
    "        mapper=RunnableParallel({\n",
    "            \"answer\": RunnableAssign(\n",
    "                mapper={\"context\": RunnableLambda(lambda state:format_docs(state['context']))}\n",
    "            )\n",
    "            | chat_prompt_extr_narr\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        })\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"What are the predictions for the future economic conditions made by this document\"\n",
    "answer = chain.invoke(QUESTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing to text in dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DIR, \"answer.txt\"), mode=\"w\") as f:\n",
    "    f.write(answer['answer'])\n",
    "with open(os.path.join(PATH_TO_DIR, \"source.txt\"), mode=\"w\") as f:\n",
    "    for d in answer['context']: \n",
    "        f.write(\"-\"*15 + \"\\n\")\n",
    "        if (source:=d.metadata['source']): \n",
    "            f.write(f\"souce: {source}\")\n",
    "        if (page:=d.metadata['page']):\n",
    "            f.write(f\"\\tpage: {page}\")\n",
    "        f.write(\"\\n\\n\" + d.page_content + \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
