{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exploring vectordatabases and retrievers\n",
    "this notebook focuses on exploring different ways to set up vector db and retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableSequence, RunnableAssign, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "import os\n",
    "from os.path import  join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this needs more work \n",
    "need_to_recompile = False\n",
    "if(not os.path.exists(\"faiss_index\") or need_to_recompile):\n",
    "    path_to_docs = './text'\n",
    "    docs = []\n",
    "    for f in os.listdir(path_to_docs):\n",
    "        file_path = join(path_to_docs, f)\n",
    "        if os.path.isfile(file_path):\n",
    "            loader = UnstructuredHTMLLoader(file_path)\n",
    "            docs.extend(loader.load())\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    vectorstore = FAISS.from_documents(splits, OpenAIEmbeddings())\n",
    "    vectorstore.save_local(\"faiss_index\")\n",
    "else:\n",
    "    vectorstore = FAISS.load_local(\"faiss_index\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=['context','question'],\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=['context', 'question'], \n",
    "                template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chain(retriever):\n",
    "  chain = (\n",
    "    #our first step is to create a dict {context: retriever.invoke($input_question), question: $input_question}, done through runnableparallel\n",
    "    RunnableParallel({\n",
    "      \"context\": retriever,\n",
    "      \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    #with this dict, we then use runnable assign to keep our original dictionary, but also add on a new field; answer                                                                                      \n",
    "    | RunnableAssign(           \n",
    "        # we add the new field with the mapper funciotn, which takes a new runnable parallel\n",
    "        # this parrallel will take the dictionary passed into it, use it as input, and when it has its output, it will add the parallels dict to the orinal\n",
    "        # {**input_dictionary, **our_parallel_function(input_dictionary)->dict}                                                      \n",
    "        mapper=RunnableParallel(\n",
    "            #we just need one thing, the answer, so that is the only field\n",
    "            {\"answer\": \n",
    "              # prompt is expecting a dict with a 'question' and 'context' field, we currently have both of those fields, but context is a list of Document objects\n",
    "              # we use the assign and format_docs function to remap context to our desired format \n",
    "              RunnableAssign(\n",
    "                  mapper={\"context\": RunnableLambda(lambda x: format_docs(x['context']))}\n",
    "                )\n",
    "              | prompt\n",
    "              | llm\n",
    "              | StrOutputParser()\n",
    "              #at the end, this parallel has produced a dict of {answer: string output}, which is then added to the original\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "  )\n",
    "  return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(chain, question):\n",
    "    ans = chain.invoke(question)\n",
    "    print(\"question = \",ans[\"question\"])\n",
    "    print(\"answer = \", ans['answer'])\n",
    "    print(\"Documents used:\")\n",
    "    for d in ans['context']:\n",
    "        if len(d.page_content) > 40:\n",
    "            print(\"\\tsource: \"+d.metadata['source']+\"\\t\"+d.page_content[:40]+\"...\"+d.page_content[-30:])\n",
    "        else:\n",
    "            print(\"\\tsource: \"+d.metadata['source']+\"\\t\"+d.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fyi on other ways to chain text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other ways to do chain \n",
    "```python\n",
    "RunnableSequence(\n",
    "    RunnableParallel({\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }),\n",
    "    RunnableAssign(           \n",
    "      mapper=RunnableParallel(\n",
    "          {\"answer\": \n",
    "            RunnableAssign(\n",
    "                mapper={\"context\": RunnableLambda(lambda x: format_docs(x['context']))}\n",
    "              )\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "          }\n",
    "      )\n",
    "    )   \n",
    ")\n",
    "############\n",
    "RunnableParallel({\n",
    "    \"context\": retriever,\n",
    "    \"question\": RunnablePassthrough()\n",
    "}).assign(answer=RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question =  who was the first president\n",
      "answer =  George Washington became the first president of the United States in 1789. The national capital moved from New York to Philadelphia in 1790 and finally settled in Washington, D.C., in 1800. Washington's presidency marked the beginning of the new federal government under the Constitution.\n",
      "Documents used:\n",
      "\tsource: ./text\\history.html\tEarly republic (1793–1830)[edit]\n",
      "\n",
      "Main a... in Washington, D.C., in 1800.\n",
      "\tsource: ./text\\history.html\tNationalists – most of them war veterans...residency of George Washington\n",
      "\tsource: ./text\\history.html\tMain article: \n",
      "\n",
      "Presidency of Richard Ni...ng to the right-center.\n",
      "\n",
      "[271]\n",
      "\tsource: ./text\\history.html\t[471]\n",
      "\n",
      "campaigning for the\n",
      "\n",
      "2024 preside...education in the United States\n"
     ]
    }
   ],
   "source": [
    "retriever  = vectorstore.as_retriever()\n",
    "chain = make_chain(retriever )\n",
    "ask_question(chain, \"who was the first president\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve more documents with higher diversity\n",
    "Useful if your dataset has many similar documents\n",
    "```python\n",
    "vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
    ")\n",
    "```\n",
    "\n",
    "Fetch more documents for the MMR algorithm to consider\n",
    "But only return the top 5\n",
    "```python\n",
    "vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 5, 'fetch_k': 50}\n",
    ")\n",
    "```\n",
    "\n",
    "Only retrieve documents that have a relevance score\n",
    "Above a certain threshold\n",
    "```python\n",
    "vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.8}\n",
    ")\n",
    "```\n",
    "\n",
    "Only get the single most similar document from the dataset\n",
    "```python\n",
    "vectorstore.as_retriever(search_kwargs={'k': 1})\n",
    "```\n",
    "\n",
    "Use a filter to only retrieve documents from a specific paper\n",
    "```python\n",
    "vectorstore.as_retriever(\n",
    "    search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
